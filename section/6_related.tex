\section{Related Works}
\label{sec:related}

\subsection{Performance Diagnosis}
Profilers are the most commonly used tools that help developers understand
performance
\cite{Zaparanuks:2012:AP:2254064.2254074,Coppa:2012:IP:2254064.2254076,
D'Elia:2011:MHC:1993498.1993559, 
Mytkowicz:2010:EAJ:1806596.1806618,
Ravindranath:2012:AMA:2387880.2387891,
Jovic:2011:CMY:2048066.2048081,IntroPerf}.
Different from our work, profiling aims to tell where 
computation resources are spent, 
not where and why computation resources are wasted. 
The root-cause code region of a performance problem often is not inside
the top-ranked function in the profiling result \cite{SongOOPSLA2014}.
Even if it is, developers still need to spend a lot of effort to understand
whether and what kind of computation inefficiency exists.

Recently, tools that go beyond profiling and provide more automated analysis
have been proposed.
StackMine~\cite{Han:2012:PDL:2337223.2337241} 
identifies slow call-stack patterns inside event handlers. The work by
\citet{TaoAsplos2014} analyzes system traces to understand 
performance impact propagation and 
causality relationship among system components. 
X-ray~\cite{Attariyan:2012:XAR:2387880.2387910} helps identify inputs
or configuration entries that are most responsible for performance problems.
Coz~\cite{coz.sosp15} can estimate the performance impact of any
potential optimization at any point of a multi-threaded program.
Although these techniques are all very useful, 
they target different problems from \Tool. 
\comment{
\Tool targets 
inefficient loops, a common
inefficiency problem that contributes to
about two thirds of real-world user-perceived performance problems studied
by previous work \cite{SongOOPSLA2014}, and tries to pin-point the exact
root cause and provide fix-strategy suggestions.
}

\Tool is most related to
the recent statistical performance debugging work
\citep{SongOOPSLA2014}, both trying to identify source-code level root causes
for user-perceived performance problems. 
This previous work identifies which loop is most correlated with 
a performance symptom through statistical analysis, 
but cannot answer whether or 
what type of inefficiency this loop contains.
\Tool complements it by
providing detailed root cause information and fix strategy suggestions.

\subsection{Performance Bug Detection}

Many dynamic and static analysis tools have been built to detect different
types of performance problems, such as 
run-time bloat~\cite{Dufour:2008:STC:1453101.1453111, Xu:2009:GFP:1542476.1542523, Xu:2010:DIC:1806596.1806616}, 
low-utility data structures~\cite{Xu:2010:FLD:1806596.1806617}, 
cacheable data~\cite{Cachetor}, false sharing in
multi-thread programs~\cite{Liu:2011:SPD:2048066.2048070},
inefficient nested loops~\cite{Alabama},
loops with unnecessary iterations~\cite{CARAMEL,IsilDillig.PLDI15},
input-dependent loops~\cite{xiao13:context}. 
%Some static tools \cite{PerfBug} identify performance defects that violate
%specific efficiency rules, often related to API usage. 

As discussed in 
Section \ref{sec:intro}, these tools are all useful in improving software
performance, but are not suitable for performance diagnosis.
With different design goals from \Tool, these bug detection tools are not 
guided by any specific
performance symptoms. Consequently, they take different coverage-accuracy
trade-offs from \Tool. \Tool tries to cover a wide variety of root-cause 
categories and is more aggressive in identifying root-cause categories, because
it is only applied to a small number of loops that are known to be highly
correlated with the specific performance symptom under study.
Performance-bug detection tools has to be more conservative and tries hard
to lower false positive rates, because it needs to process the
whole program, instead of just a few loops. This requirement causes bug
detection tools to each focus on a specific root-cause category.
%In fact, most inefficient loops in our study are very difficult to be detected
%by a generic performance bug detectors --- it is very difficult to prove them to
%be inefficient without application-specific domain knowledge.
%For example, only a couple of inefficient loops out of the 45 studied
%in Table \ref{tab:root} can be detected by recent static inefficient
%loop checkers
%\cite{CARAMEL,IsilDillig.PLDI15}.
Performance bug detection tools also do not aim to provide fix suggestions.
For the few that do provide \cite{CARAMEL}, they only focus on very specific
fix patterns, such as adding a break into the loop. 
In addition, dynamic performance bug detectors do not try to achieve any
performance goals. None of them has tried applying sampling to their
bug detection and often leads to 10X slowdowns or more.

\subsection{Other Techniques to Fight Performance Bugs}

There are many test inputs generation techniques that help
performance testing 
\cite{WISE, EventBreak, SpeedGun}.
Some techniques aim to improve the test selection or prioritization during 
performance testing~\cite{Forepost,Huang:2014:PRT:2568225.2568232}.  
All these techniques combat performance bugs from different aspects from performance diagnosis.
