\section{Evaluation of \Tool}
\label{sec:experiment}

\subsection{Methodology}
\label{sec:result_meth}
%Please discuss the potential usage scenarios of \Tool
%How you will use it together with other tools 

\input section/tab_benchmarks

\paragraph{Implementation and Platform}
We implement \Tool in LLVM-3.4.2 \cite{llvm}, and conduct our
%Our implementation consists of 17654 lines of C++ code, 
%with 4748 for instrumenter, 674 for resultless analysis, 
%5802 for redundancy analysis, 
%and the remaining 6430 for common utilities. 
experiments on a i7-960 machine, with Linux 3.11 kernel. 

\paragraph{Benchmarks}
Note that \Tool is a tool that helps diagnose performance problems that
have already manifested, not a detection tool that can help predict not-yet-manifested
problems. Consequently, our benchmarks are performance problems that have already
happened in real world, and we will reproduce these problems to evaluate \Tool.
To conduct a thorough evaluation, we use
benchmarks from two sources.

First, we evaluate \Tool on 18 out of the 45 bugs listed in Table \ref{tab:root}. 
Among these 18, seven are extracted from Java or JavaScript
programs and re-implemented in C++, as \Tool currently only handles C/C++
programs; one is extracted from a very old version of Mozilla.
%These 18 bugs include all the benchmarks used in the recent
%statistical performance debugging work~\cite{SongOOPSLA2014}.
The extraction is conducted by XXX.
We did not use the remaining 27 bugs listed in Table \ref{tab:root}
as benchmarks, 
either because they depend on special hardware/software environment
or because they involve too complex data structures to extract XXX. 
Overall, these 18 bugs cover a wide variety of inefficiency root causes, as 
shown in Table~\ref{tab:benchmarks}. 

Second, we evaluate \Tool on 21 out of the xx bugs in the benchmark
suite of Toddler \cite{Alabama}.
All of these bugs come from Java programs. We extract XXX (how did you
extract exactly?) and re-implement in C++. Each extracted benchmark 
contains 4--5 loops (how come you always have 4--5 loops??XXX). 
Note that, Toddler focuses on inefficient nested loops, and its
benchmark bugs only cover two types of inefficiency root causes,
as shown in Table \ref{tab:benchmarks}.
We use Toddler benchmark suite because it provides a larget set of
repeatable inefficient loop problems. 


\paragraph{Metrics}
Our experiments are designed to evaluate \Tool from three main aspects:
(1) 
\textit{Coverage}. Given our benchmark suite that covers a wide variety
of real-world root causes, can \Tool identify all those root causes?
(2)
\textit{Accuracy}. 
When analyzing non-buggy loops, will \Tool generate any false positives?
(3) 
\textit{Performance}.
What is the run-time overhead of \Tool?

\paragraph{Evaluation settings}
Our evaluation uses existing statistical performance diagnosis
tool \cite{SongOOPSLA2014} to process a performance problem and identify 
one or a few suspicious loops for \Tool to analyze.
For 14 out of the 18 benchmarks, statistical debugging identifies the
real root-cause loop as the most suspicious loop. For the remaining
benchmarks, the real root-cause loops are ranked number 2, 2, 4, and 10.
%Overall, we believe future tools can accurately identify the most one or a couple
%of suspicious loops.
XXX (what can we say about Toddler benchmark set here?)

\input section/tab_cover

To evaluate the coverage, accuracy, and performance of \Tool, we mainly conduct
three sets of evaluation. First, we apply \Tool to the real root-cause loop to
see if \Tool can correctly identify the root-cause category and provide
correct fix-strategy suggestion. Second, we apply
statistical performance debugging \cite{SongOOPSLA2014} to all our benchmarks
and apply \Tool to the top 5 ranked loops\footnote{Some extracted benchmarks
have fewer than 5 loops. We simply apply \Tool to all loops in these cases.}
to see how accurate \Tool is. Third, we evaluate the run-time performance of
applying \Tool to the real root-cause loop. 
 
For all benchmarks we use, real-world
users have provided at least one problem-triggering input in their on-line 
bug bugs. We use these inputs in our run-time analysis.

As discussed in Section \ref{sec:design}, our analysis contains 
several configurable thresholds. In our evaluation,
we use 0.001 as the \textit{resultful rate} threshold for identifying
0*1? 
%loops, 0.01 as the \textit{resultful rate} threshold for identifying 
and [0$|$1]* resultless loops; we use 
0.5 as the \textit{redundancy rate} threshold for identifying redundant loops.
%, and 
%2 as the \textit{cross-iteration redundancy rate} (i.e., 
%the number of distinct iterations is less than half of the total iterations).

All the analysis and performance results presented below regarding
cross-loop analysis is obtained using $1/100$ sampling rate; all the
results regarding cross-iteration analysis is obtained using $1/1000$ sampling
rate. We use sparser sampling rate in the latter case, because there tend to
be more loop iterations than loop instances.
All our diagnosis results require only \textbf{one} run under the 
problem-triggering input.

More discussions about all the parameters/thresholds presented above, including
how to set them and how sensitive they are, are discussed in Section
\ref{sec:sensi}. 

\subsection{Coverage Results}
\label{sec:coverage}
Overall, \Tool provides good diagnosis coverage, as shown in Table~\ref{tab:cover}. 
\Tool identifies the correct root cause for \textbf{all} \allbugs benchmarks, and 
suggests fix strategies that exactly match what developers took in practice
for 33 out of \allbugs cases. 

The six cases where the fix strategy suggested by \Tool does not match that of 
developers fall into three categories.
First, the fix strategy taken by developers is a subset of what suggested by 
\Tool.
For MySQL\#27287 and Apache\#53622, the root-cause loop
is both cross-loop redundant and 0*1? inefficient. \Tool suggests both changing
data structures and memoization as fix strategies. In practice, the developers
only changed the data structure, which eliminated both types of inefficiency,
in case of MySQL\#27287, or the main type of inefficiency, in case of Apache\#53622.

Second, the fix strategy taken by developers is a superset of what suggested by
\Tool.
In Collections bugs \#407, \#408, and \#425, XXX.

Third, \Tool cannot suggest fix strategy for 1* loops.
For GCC\#12322, \Tool correctly tells that the loop under study
does not contain any form of inefficiency and produce results in every 
iteration, and hence fails to suggest any fix strategy. In practice, GCC
developers decide to skip the loop, which will cause some programs compiled by
GCC
to be less performance-optimal than before. However, GCC developers feel
that it is worthwhile considering the slow-down caused by the original loop.

\subsection{Accuracy Results}
\label{sec:result_acc}

\input section/tab_top5

As shown in Table \ref{tab:top5}, \Tool is accurate, having 0 real
false positive and 23 benign false positives for all the top 5 loops
of the \allbugs benchmarks.

Here, benign false positives mean that the \Tool analysis result is true ---
some loops are indeed cross-iteration/loop redundant or indeed producing
results in only a small portion of all the iterations. However, those
problems are \textit{not} fixed by developers in their performance patches. 

There are several reasons for these benign performance problems. 
The main reason is that they are not the main contributor to the 
performance problem perceived by the users. This happens to 11 out of the
13 benign cases (XXX subject to change XXX). 
In fact, this is not really a problem for \Tool in 
real usage scenarios, because statistical debugging can accurately
tell that these loops are not top contributors to the performance
problems.
Two cases happen when fixing the 
identified redundant/resultless problems
are very difficult and hence developers decide not to fix them.
The remaining cases XXX.


The accuracy of \Tool benefits from its run-time analysis.
For example, there are 7 (XXX to change XXX) benchmarks in total that each contains
a loop that generates side effect only
in its last iteration. Without run-time information, \Tool would judge
all of them as inefficient (0*1? resultless). Fortunately,
\Tool run-time counts the total number of iterations and
correctly identifies 3 (XXX to change XXX) of them as truly inefficient and severe enough
to cause the corresponding performance problem.

\Tool can also help improve the accuracy of statistical debugging in
identifying which loop is the root-cause loop.
For example, the real root-cause loop of Apache\#34464 and GCC\#46401 both
rank number two by the statistical performance diagnosis tool.
Fortunately,
\Tool can tell that the number one loops in both cases do not contain
any form of inefficiency, resultless or redundancy. 

\subsection{Performance}
\label{sec:result_perf}

\begin{table}
  \centering
  \scriptsize
  \newcommand{\Yes}[1]{\checkmark{}$_#1$}
  \newcommand{\No}[0]{-}
  \begin{tabular}{lccccc}
    \toprule
	    & \multicolumn{3}{c}{\Tool w/ optimization} & \multicolumn{2}{c}{w/o optimization} \\
     \cmidrule(lr){2-4}
     \cmidrule(lr){5-6}
     {\bf BugID}  & {\bf Resultless}  &  {\bf C-L R. } & {\bf C-I R. }  & {\bf C-L R.}  & {\bf C-I R. } \\
    \midrule
    Mozilla347306 &  1.07\%           &  22.40\%       &  10.17\%       & 304.37{\bf X} & 468.74{\bf X} \\ 
    Mozilla416628 &  0.80\%           &  4.10\%        &  2.99\%        & 567.51{\bf X} & 85.6{\bf X} \\
    \midrule
     MySQL27287   & $<$0.01\%           &   1.66\%       &   -            & 109.55{\bf X} & 352.07{\bf X} \\
     MySQL15811   &  -                &   0.03\%       &   -            & 227.04{\bf X} & 424.44{\bf X} \\
    \midrule
      GCC46401    & 3.12\%         & 3.80\%            &  5.95\%        & 21.07{\bf X}  & 38.44{\bf X}\\ 
      GCC1687     & -              & /                 &  $<$0.01\%       &   /           & 142.29{\bf X} \\
      GCC27733    & $<$0.01\%        & /                 &  4.73\%        &   /           & 17.41{\bf X}     \\
      GCC8805     & -              & $<$0.01\%           & $<$0.01\%        & 2.22{\bf X}   &  3.52{\bf X}\\
      GCC21430    & -              & 5.46\%            &   0.69\%       & 107.20{\bf X} & 159.89{\bf X} \\
      GCC12322    & -              & 1.75\%            &  $<$0.01\%       & 21.07{\bf X}  & 38.44{\bf X} \\
   \bottomrule
   \end{tabular}
  %\nocaptionrule
  \caption{Run-time overhead of applying \Tool to the buggy loop
    (only non-extracted benchmarks are shown). 
  -: dynamic analysis is not needed;
  /: not applicable.}
  \label{tab:performance}
\end{table}


As shown in Table \ref{tab:performance}, 
the performance of \Tool is good. The overhead is consistently under or around 5\% 
except for one benchmark, Mozilla\#347306. We believe \Tool is promising for potential production
run usage.
We can easily further lower the overhead through sparser sampling.
As we will discuss later, 
the current diagnosis results are obtained by running the
program only \textbf{once} under the problem-triggering workload.
If we use sparser sampling, more failure runs will be needed to obtain good
diagnosis results.

As we can also see from the table, our performance optimization discussed in 
Section \ref{sec:inst} and \ref{sec:perf} has helped.

The performance benefit of sampling is huge.
Without sampling, even with all the static optimization, redundancy
analysis lead to over 100X slowdown for five benchmarks.

The benefit of static optimization is also non-trivial. 
For example, for MySQL\#15811 and MySQL\#27287, static analysis alone can
judge that they do not contain cross-iteration redundancy: the computation of 
each iteration depends on loop induction variables, which are naturally different
in different iterations. Consequently, run-time overhead is completely
eliminated for these two benchmarks.
As another example, the buggy loops of MySQL\#27287 and MySQL\#15811 access 
arrays. 
After changing to tracking the initial and ending memory-access addresses
of the array, instead of the content of the whole array accesses,
the overhead is reduced from 11.77\% to 1.66\% for MySQL\#27287, 
and from 20.46\% to 0.03\% for MySQL\#15811 respectively 
(sampling is conducted consistently here). 

\subsection{Parameter Setting and Sensitivity}
\label{sec:sensi}
\paragraph{Sampling rates}
We have tried different sampling rates for redundancy analysis.
Intuitively, sparser sampling leads to lower overhead but worse diagnosis
results. Due to space constraints, we briefly summarize the results below.

When we lower the sampling rate from 1/100 to 1/1000 
in cross-loop redundancy analysis,
among all the benchmarks in Table \ref{tab:performance},
Mozilla\#347306 still incurs the largest overhead (0.49\%). 
The diagnosis results remain the same for all but
GCC\#12322, where too few samples are available
to judge redundancy.

When we lower the sampling rate from 1/1000 to 1/10000
in cross-iteration redundancy analysis,
among all the benchmarks in Table \ref{tab:performance},
Mozilla\#347306 has the largest overhead (4.47\%). 
%In fact, Mozilla347306 is the only one whose overhead is larger than 2\%. 
The diagnosis results remain 
the same for all but GCC\#12322.
%If we increase the sampling rate to 1/100, we will 
%have two benchmarks whose overhead is larger than 30\%.

\paragraph{Resultful and redundancy rate}
Since the severity of resultless and redundant loops depends on
workload, it is natural that \Tool uses two thresholds in its diagnosis.
In fact, the diagnosis results are largely insensitive to the threshold
setting. For example, the results would remain the same when
changing the redundancy rate threshold from 0.5 to any value between about
0.1 and 0.66. We will have 1 more false negative and 1 fewer benign false positive, 
when the rate is 0.7. The trend is similar for resultless
loop checking. 
XXX

Our default setting %is obtained based on our experience
should work for many problems.
Developers can adjust these thresholds. 
They can even get rid of thresholds, and only
use the raw values of resultful/redundancy rates to understand
the absolute and relative (in)efficiency nature of suspicious 
loops. Based on our experiments, the difference between efficient and inefficient
loops is usually obvious, based on these rates.

\subsection{Comparison with other tools}
XXXXXX
