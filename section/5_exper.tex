\section{Evaluation of \Tool}
\label{sec:experiment}

\subsection{Methodology}
\label{sec:result_meth}
%Please discuss the potential usage scenarios of \Tool
%How you will use it together with other tools 

\input section/tab_benchmarks

\paragraph{Implementation and Platform}
We implement \Tool in LLVM-3.4.2 \cite{llvm}, and conduct our
%Our implementation consists of 17654 lines of C++ code, 
%with 4748 for instrumenter, 674 for resultless analysis, 
%5802 for redundancy analysis, 
%and the remaining 6430 for common utilities. 
experiments on a i7-960 machine, with Linux 3.11 kernel. 

\paragraph{Benchmarks}
Note that \Tool is a tool that helps diagnose performance problems that
have already manifested, not a detection tool that can help predict not-yet-manifested
problems. Consequently, our benchmarks are performance problems that have already
happened in real world, and we will reproduce these problems to evaluate \Tool.
To conduct a thorough evaluation, we use
benchmarks from two sources.

First, we evaluate \Tool on all bugs, 18 in total, that we can reproduce 
among the 45 bugs listed in Table \ref{tab:root}. 
Among these 18, seven are extracted from Java or JavaScript
programs and re-implemented in C++, as \Tool currently only handles C/C++
programs; one is extracted from a very old version of Mozilla that can no longer be
compiled as it is.
%These 18 bugs include all the benchmarks used in the recent
%statistical performance debugging work~\cite{SongOOPSLA2014}.
%The extraction is conducted by XXX.
When we extract code, we try our best to keep all bug-related data structures,
caller functions, and callee functions intact, and re-implement them in C++, 
following the original data flow and control flow. For these extracted benchmarks, 
we design bug-triggering inputs for them based on the bug-triggering inputs in 
the original bug reports and our understanding about how original program inputs
map to the inputs of our re-implemented programs.
Overall, these 18 bugs cover a wide variety of inefficiency root causes, as 
shown in Table~\ref{tab:benchmarks}. 
We will refer to this set of bugs as our \emph{general} benchmark suite.

Note that, we expect \Tool algorithm to also work for the remaining 27 bugs 
listed in Table \ref{tab:root}, but they are too difficult to be reproduced
on our machines.
First, they all depend on 
special hardware/software environment that our machine is not equipped with.
For example, some bugs can only be triggered on Windows, and some bugs depend on 
.Net libraries. 
Furthermore, the bug-related code for these 27 cases
cannot be easily extracted or reimplemented, as we did for the eight 
benchmarks mentioned above. 
For example, some bugs are related to GUI widgets, which are too complicated to 
re-implement or emulate. As another example, some bugs' inefficient 
loops traverse big graphs. Although we can extract the buggy code regions, it is 
too difficult to figure out the bug-triggering inputs
(i.e., the graphs) without reproducing the original bugs. 

%Second, we evaluate \Tool on 21 out of the 42 detected by Toddler \cite{Alabama, toddlerbuglist}.
%{\color{red} We use all benchmarks that are reported with bug-triggering inputs and fixed by developers. 
%All of these bugs come from Java programs. 
%We extract XXX (how did you
%extract exactly?) and re-implement in C++. Each extracted benchmark 
%contains 4--5 loops (how come you always have 4--5 loops??XXX). 
Second, we evaluate \Tool on Toddler~\cite{Alabama, toddlerbuglist} benchmark suite.
Toddler project provides the bug-triggering inputs and detailed explanation
for 21 inefficient-loop bugs that have been confirmed and fixed by developers, and
we evaluate \Tool on \emph{all} of these 21 bugs.
Due to the focus of Toddler, all of these bugs 
are related to misusing basic data structures in Java, like ArrayList, HashSet, 
HashMap, and LinkedList. We extract these bugs and re-implemented them in 
C/C++, following the procedure discussed above. In doing so,
we also re-implement basic Java data structures following a recent version of 
\texttt{openjdk}. At the end, each extracted benchmark contains at least five
loops, except for two cases where only 4 loops exist in the extracted version. 
Note that, Toddler focuses on inefficient nested loops, and hence its
benchmark bugs only cover two types of inefficiency root causes,
as shown in Table \ref{tab:benchmarks}.

We use Toddler benchmark suite for two reasons.
First, it was set up for completely different reasons and methodology from our
\emph{general} benchmark suite,
and hence can well complement the latter suite.
Second, it provides the largest set of 
repeatable inefficient loop problems that we can find. 

\paragraph{Metrics}
Our experiments are designed to evaluate \Tool from three main aspects:
(1) 
\textit{Coverage}. Given our benchmark suite that covers a wide variety
of real-world root causes, can \Tool identify all those root causes?
(2)
\textit{Accuracy}. 
When analyzing non-buggy loops, will \Tool generate any false positives?
(3) 
\textit{Performance}.
What is the run-time overhead of \Tool?

\paragraph{Evaluation settings}
Our evaluation uses existing statistical performance diagnosis
tool \cite{SongOOPSLA2014} to process a performance problem and identify 
one or a few suspicious loops for \Tool to analyze.
For all but four benchmarks, statistical debugging identifies the
real root-cause loop as the most suspicious loop. For the remaining four
benchmarks, which all come from Table \ref{tab:root},
the real root-cause loops are ranked number 2, 2, 4, and 10.
%Overall, we believe future tools can accurately identify the most one or a couple
%of suspicious loops.
%XXX (what can we say about Toddler benchmark set here?)

\input section/tab_cover

To evaluate the coverage, accuracy, and performance of \Tool, we mainly conduct
three sets of evaluation. First, we apply \Tool to the real root-cause loop to
see if \Tool can correctly identify the root-cause category and provide
correct fix-strategy suggestion. Second, we apply
statistical performance debugging \cite{SongOOPSLA2014} to all our benchmarks
and apply \Tool to the top 5 ranked loops\footnote{Some extracted benchmarks
have fewer than 5 loops. We simply apply \Tool to all loops in these cases.}
to see how accurate \Tool is. Third, we evaluate the run-time performance of
applying \Tool to the real root-cause loop. 
 
For all benchmarks we use, real-world
users have provided at least one problem-triggering input in their on-line 
bug bugs. We use these inputs in our run-time analysis.

As discussed in Section \ref{sec:design}, our analysis contains 
several configurable thresholds. In our evaluation,
we use 0.001 as the \textit{resultful rate} threshold for identifying
0*1? 
%loops, 0.01 as the \textit{resultful rate} threshold for identifying 
and [0$|$1]* resultless loops; we use 
0.5 as the \textit{redundancy rate} threshold for identifying redundant loops.
%, and 
%2 as the \textit{cross-iteration redundancy rate} (i.e., 
%the number of distinct iterations is less than half of the total iterations).

All the analysis and performance results presented below regarding
cross-loop analysis is obtained using $1/100$ sampling rate; all the
results regarding cross-iteration analysis is obtained using $1/1000$ sampling
rate. We use sparser sampling rate in the latter case, because there tend to
be more loop iterations than loop instances.
All our diagnosis results require only \textbf{one} run under the 
problem-triggering input.

More discussions about all the parameters/thresholds presented above, including
how to set them and how sensitive they are, are discussed in Section
\ref{sec:sensi}. 

\subsection{Coverage Results}
\label{sec:coverage}
Overall, \Tool provides good diagnosis coverage, as shown in Table~\ref{tab:cover}. 
\Tool identifies the correct root cause for \textbf{all} \allbugs benchmarks, and 
suggests fix strategies that exactly match what developers took in practice
for 33 out of \allbugs cases. 

The six cases where the fix strategy suggested by \Tool does not match that of 
developers fall into three categories.
First, the fix strategy taken by developers is a subset of what suggested by 
\Tool.
For MySQL\#27287 and Apache\#53622, the root-cause loops contain both
cross-loop redundancy and 0*1? inefficiency. \Tool suggests both changing
data structures and memoization as fix strategies. In practice, the developers
acknowledge both types of inefficiencies, but the patches
only changed the data structures, which eliminated both types of inefficiencies 
in case of MySQL\#27287 and left the cross-loop redundancy unsolved in
Apache\#53622. 
%For Apache\#53622, there are two buggy loops, one is cross-loop redundant, and the other is 0*1? inefficient. 
%\Tool suggests batching and changing data structure, respectively. 
%In practice, developers only fix the main root cause loop, by using changing data structure.} 

Second, the fix strategy taken by developers is related but not exactly the same as
what suggested by \Tool.
The root-cause loops in 
Collections bugs \#407, \#408, and \#425 all conduct frequent linear searches in arrays.
\Tool suggests data-structure changes for these three cases. Developers' patches 
still keep the original data structure and the original buggy loop, but they
did use hash-sets, which contain the same content as the arrays do,
to help conditionally skip the loops. 

Third, \Tool cannot suggest fix strategy for 1* loops.
For GCC\#12322, \Tool correctly tells that the loop under study
does not contain any form of inefficiency and produce results in every 
iteration, and hence fails to suggest any fix strategy. In practice, GCC
developers decide to skip the loop, which will cause some programs compiled by
GCC
to be less performance-optimal than before. However, GCC developers feel
that it is worthwhile considering the slow-down caused by the original loop.

\subsection{Accuracy Results}
\label{sec:result_acc}

\input section/tab_top5

As shown in Table \ref{tab:top5}, \Tool is accurate, having 0 real
false positive and 22 benign false positives for all the top 5 loops
of the \allbugs benchmarks.

Here, benign false positives mean that the \Tool analysis result is true ---
some loops are indeed cross-iteration/loop redundant or indeed producing
results in only a small portion of all the iterations. However, those
problems are \textit{not} fixed by developers in their performance patches. 

There are several reasons for these benign performance problems. 
The main reason is that they are not the main contributor to the 
performance problem perceived by the users. This happens to 20 out of the
22 benign cases. % 11 out of 13 in the first benchmark set 
In fact, this is not really a problem for \Tool in 
real usage scenarios, because statistical debugging can accurately
tell that these loops are not top contributors to the performance
problems.
Two cases happen when fixing the 
identified redundant/resultless problems
are very difficult and hence developers decide not to fix them.

The accuracy of \Tool benefits from its run-time analysis.
%{\color{red} For example, there are 26 
For example, our run-time analysis has correctly pruned out 24 false positives
in 0*1? inefficiency detection for our benchmarks. Each of these 24 loops is a
top-5 suspicious loops in one of our benchmarks; it only generates side effect
in its last iteration, and hence is identified as by \Tool static analysis. 
Without run-time information, \Tool would judge
all of them as inefficient (0*1? resultless). Fortunately,
\Tool run-time counts the total number of iterations and
correctly identifies them as false positives. Similarly, \Tool run-time analysis
helps prune out 19 false positives for [0$|$1]* loop identification.

\Tool can also help improve the accuracy of statistical debugging in
identifying which loop is the root-cause loop.
For example, the real root-cause loop of Apache\#34464 and GCC\#46401 both
rank number two by the statistical performance diagnosis tool.
Fortunately,
\Tool can tell that the number one loops in both cases do not contain
any form of inefficiency, resultless or redundancy. 

\subsection{Performance}
\label{sec:result_perf}

\begin{table}
  \centering
  \scriptsize
  \newcommand{\Yes}[1]{\checkmark{}$_#1$}
  \newcommand{\No}[0]{-}
  \begin{tabular}{lccccc}
    \toprule
	    & \multicolumn{3}{c}{\Tool w/ optimization} & \multicolumn{2}{c}{w/o optimization} \\
     \cmidrule(lr){2-4}
     \cmidrule(lr){5-6}
     {\bf BugID}  & {\bf Resultless}  &  {\bf C-L R. } & {\bf C-I R. }  & {\bf C-L R.}  & {\bf C-I R. } \\
    \midrule
    Mozilla347306 &  1.07\%           &  22.40\%       &  10.17\%       & 304.37{\bf X} & 468.74{\bf X} \\ 
    Mozilla416628 &  0.80\%           &  4.10\%        &  2.99\%        & 567.51{\bf X} & 85.6{\bf X} \\
    \midrule
     MySQL27287   & $<$0.01\%           &   1.66\%       &   -            & 109.55{\bf X} & 352.07{\bf X} \\
     MySQL15811   &  -                &   0.03\%       &   -            & 227.04{\bf X} & 424.44{\bf X} \\
    \midrule
      GCC46401    & 3.12\%         & 3.80\%            &  5.95\%        & 21.07{\bf X}  & 38.44{\bf X}\\ 
      GCC1687     & -              & /                 &  $<$0.01\%       &   /           & 142.29{\bf X} \\
      GCC27733    & $<$0.01\%        & /                 &  4.73\%        &   /           & 17.41{\bf X}     \\
      GCC8805     & -              & $<$0.01\%           & $<$0.01\%        & 2.22{\bf X}   &  3.52{\bf X}\\
      GCC21430    & -              & 5.46\%            &   0.69\%       & 107.20{\bf X} & 159.89{\bf X} \\
      GCC12322    & -              & 1.75\%            &  $<$0.01\%       & 21.07{\bf X}  & 38.44{\bf X} \\
   \bottomrule
   \end{tabular}
  %\nocaptionrule
  \caption{Run-time overhead of applying \Tool to the buggy loop
    (only non-extracted benchmarks are shown). 
  -: dynamic analysis is not needed;
  /: not applicable.}
  \label{tab:performance}
\end{table}


As shown in Table \ref{tab:performance}, 
the performance of \Tool is good. The overhead is consistently under or around 5\% 
except for one benchmark, Mozilla\#347306. We believe \Tool is promising for potential production
run usage.
We can easily further lower the overhead through sparser sampling.
As we will discuss later, 
the current diagnosis results are obtained by running the
program only \textbf{once} under the problem-triggering workload.
If we use sparser sampling, more failure runs will be needed to obtain good
diagnosis results.

As we can also see from the table, our performance optimization discussed in 
Section \ref{sec:inst} and \ref{sec:perf} has helped.

The performance benefit of sampling is huge.
Without sampling, even with all the static optimization, redundancy
analysis lead to over 100X slowdown for five benchmarks.

The benefit of static optimization is also non-trivial. 
For example, for MySQL\#15811 and MySQL\#27287, static analysis alone can
judge that they do not contain cross-iteration redundancy: the computation of 
each iteration depends on loop induction variables, which are naturally different
in different iterations. Consequently, run-time overhead is completely
eliminated for these two benchmarks.
As another example, the buggy loops of MySQL\#27287 and MySQL\#15811 access 
arrays. 
After changing to tracking the initial and ending memory-access addresses
of the array, instead of the content of the whole array accesses,
the overhead is reduced from 11.77\% to 1.66\% for MySQL\#27287, 
and from 20.46\% to 0.03\% for MySQL\#15811 respectively 
(sampling is conducted consistently here). 

\subsection{Parameter Setting and Sensitivity}
\label{sec:sensi}
\paragraph{Sampling rates}
We have tried different sampling rates for redundancy analysis.
Intuitively, sparser sampling leads to lower overhead but worse diagnosis
results. Due to space constraints, we briefly summarize the results below.

When we lower the sampling rate from 1/100 to 1/1000 
in cross-loop redundancy analysis,
among all the benchmarks in Table \ref{tab:performance},
Mozilla\#347306 still incurs the largest overhead (0.49\%). 
The diagnosis results remain the same for all but
GCC\#12322, where too few samples are available
to judge redundancy.

When we lower the sampling rate from 1/1000 to 1/10000
in cross-iteration redundancy analysis,
among all the benchmarks in Table \ref{tab:performance},
Mozilla\#347306 has the largest overhead (4.47\%). 
%In fact, Mozilla347306 is the only one whose overhead is larger than 2\%. 
The diagnosis results remain 
the same for all but GCC\#12322.
%If we increase the sampling rate to 1/100, we will 
%have two benchmarks whose overhead is larger than 30\%.

\paragraph{Resultful and redundancy rate}
Since the severity of resultless and redundant loops depends on
workload, it is natural that \Tool uses two thresholds in its diagnosis.
In fact, the diagnosis results are largely insensitive to the threshold
setting. For example, the results would remain the same when
changing the redundancy rate threshold from 0.5 to any value between about
0.1 and 0.66. We will have 1 more false negative and 1 fewer benign false positive, 
when the rate is 0.7. The trend is similar for resultless
loop checking. 
%XXX

Our default setting %is obtained based on our experience
should work for many problems.
Developers can adjust these thresholds. 
They can even get rid of thresholds, and only
use the raw values of resultful/redundancy rates to understand
the absolute and relative (in)efficiency nature of suspicious 
loops. Based on our experiments, the difference between efficient and inefficient
loops is usually obvious, based on these rates.

\subsection{Comparison with other tools}

Automated tools have been developed to detect inefficient loop bugs
\cite{Alabama, CARAMEL, IsilDillig.PLDI15}. 
As discussed in Section \ref{sec:intro}, although these tools are very
useful, they are not suitable for failure diagnosis. 
For example, Toddler~\cite{Alabama} only targets inefficient nested loops, and
hence can only cover about half of the bugs in our \emph{general} benchmark suite.
Being a dynamic tool, it also incurs 10X or more slowdowns, which is much more
expensive than \Tool. 
Caramel~\cite{CARAMEL} statically detects inefficient loops that can be fixed
by adding conditional-breaks. Very few bugs in Table \ref{tab:} are like this.
CLARITY~\cite{IsilDillig.PLDI15} statically detects redundant traversal bugs. 
We expect it to automatically detect a sub-set of the cross-loop redundancy
loops in our benchmark set, but not for other types of inefficient
loops. In summary, \Tool aims to explain performance problems that have 
already manifested,
yet bug detection tools aim to predict problems that may not have manifested.
\Tool achieves higher generality and accuracy at the cost of the prediction
capability.

Comparing with existing profiling and performance
statistical debugging technique \cite{SongOOPSLA2014}, \Tool provides much
more detailed root-cause information. Instead of just identifying loops that
are highly correlated-with/responsible-for the execution slowness, \Tool
can accurately point out whether these suspicious loops are efficient or not,
and which type of inefficiency a loop contains, if any, and make fix suggestions.
