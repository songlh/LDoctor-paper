\section{\Tool design}
\label{sec:design}
\Tool consists of a series of analysis that
judges whether a given loop belongs to any root cause type discussed in
Section \ref{sec:study_tax}.
Its design
follows the following principles.
\begin{itemize}
\item Failure diagnosis, not bug detection. \Tool will be  
used together with other performance diagnosis tools \cite{SongOOPSLA2014}
and focus on a small
number of loops that are most correlated with a specific performance symptom,
instead of being applied to the whole program. Therefore, we will have different
design trade-offs in terms of coverage and accuracy, comparing with 
bug detection tools.

\item Static-dynamic hybrid analysis. As we will see, static analysis alone
will not be able to provide all the needed information to judge whether
a loop is problematic. However, 
dynamic analysis alone will incur too much overhead.
Therefore, we use a hybrid approach throughout our design.

\item Sampling. Loop-related 
performance problems have the unique nature of repetitiveness, which make 
them a natural fit for random sampling. We will design different
sampling schemes for different analysis.
\end{itemize}

\subsection{Resultless Checker}
\label{sec:workless}

Our resultless checker includes two parts. First, we use static analysis
to figure out which are the side-effect instructions in a loop and hence
decide whether a loop belongs to 0*, 0*1?, [0$|$1]*, or 1*. Second, to
accurately determine 0*1? and [0$|$1]* loops, we use
dynamic analysis to figure out what portion of loop iterations are
resultless at run time.
%, which will help decide whether the loop is indeed
%inefficient.

%\begin{figure}[ht]
%\center
%\includegraphics[width=0.7\linewidth]{figures/workless.pdf}
%\caption{Examples for different types of basic blocks inside a loop}
%\label{fig:block_type}
%\end{figure}

\subsubsection{Static Analysis}
\label{sec:s_workless}

\Tool first identifies side-effect instructions that write to
variables defined outside the loop. 
\Tool analyzes all functions that are called
by a loop directly or indirectly --- a function $F$ that updates variables
defined outside $F$ makes the corresponding call statement in $F$'s
caller a side-effect instruction.
We consider all library functions or function calls through function pointers 
as functions that have side effects, 
unless the library functions are specially marked by us in a white list.

\Tool then categorizes loops into four types.
Loop 0* contains no side-effect instructions. 
Loop 1* contains at least one side-effect instruction along every path that
starts from the loop header and ends at the loop header.
For the remaining cases, 
when the basic block with side-effect instructions
is part of the natural loop, the case
belongs to [0$|$1]*; instead, if the side-effect basic block is post-dominated
by one of the loop-exit blocks and is dominated by the loop header, yet is
not part of the natural loop, the case belongs to 0*1?.

Note that since the 1* pattern contains the least amount of information
about computation \textit{inefficiency}, \Tool will not report a loop's
root-cause type as 1*, if more informative root-cause type is identified 
for this loop (e.g., cross-iteration or cross-loop redundancy).

%TODO Shan will rewrite the next two paragraphs
\subsubsection{Dynamic Monitoring}
\label{sec:d_workless}

Except for 0*, none of the other three type of loops are inefficient for sure.
We need dynamic analysis to figure out what portion of loop iterations are
resultless at run time, which will help decide whether the loop worths fixing.

For a 0*1? loop, since it only generates results in the last iteration, we 
only need to know the total number of loop iterations 
to figure out the 
\textit{resultless rate} of the loop. The implementation is straightforward
--- we initialize a local counter to be 0 in the pre-header of the loop; we 
increase the counter by 1 in the loop header to count the number of 
iterations; we dump that counter to log when the loop exits.

For [0$|$1]*, we need to count not only the total number of iterations, but
also the exact number of iterations that execute side-effect instructions
at run time. 
To do that, our instrumentation uses a local boolean variable 
\texttt{HasResult} to represent whether one iteration have side effect or not. 
\texttt{HasResult} is set to \texttt{False} in the loop header, and set to
\texttt{True} after each side-effect instruction. It will be used to help
count the number of side-effect iterations. For performance concerns,
before instrumenting side-effect blocks, we check whether there are 
post-domination relation between each pair of side-effect blocks. 
If both block A and block B are side-effect blocks and block A post-dominates 
block B, we only instrument block A to update \texttt{HasResult}. 

\comment{
We could speed up the above counting using sampling. However, since the 
run-time overhead of the above counting is low, as shown in Section 
\ref{sec:experiment}, our current prototype of \Tool does 
not use sampling for this part of run-time analysis.
}

%\subsubsection{Sampling}
%We calculate the average iteration number 
%and the ratio of working iteration based on a separated 
%process from on-line branch sampling discussed in~\cite{SongOOPSLA2014}.
%We need to instrument the buggy program and re-execute it by using bug-triggering input. 
%In the future, we could design an algorithm to calculate the two resultless metrics 
%based on branch sampling reports to get rid of the extra instrumentation and the extra bad run.   

\comment{
\subsubsection{Limitations}
\label{sec:l_workless}
When callee may have side effect, we will consider it will have side effect in the caller side, 
and do not consider the real execution inside callee. This could bring false negatives, 
because we could miss resultless cases, 
where side effect instructions inside callee do not execute. 
Experiments results in Section~\ref{sec:experiment} show that this is not a big issue, 
since we do not miss any resultless bugs.  

Second, our dynamic instrumentation does not consider concurrent execution of the monitored loop, 
because most of buggy loops we study only execute in one single thread. 
When the monitored loop is executed in multi-thread, like loop marked with omp pragma, we need to synchronize updates to global variables. 
}
