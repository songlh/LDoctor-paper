\section{Introduction}
\label{sec:intro}
\subsection{Motivation}

Performance bugs\footnote{We also refer performance bugs as performance problems
following previous works in this area~\cite{PerfBug,Alabama,SongOOPSLA2014}} 
are software implementation mistakes that cause unnecessary performance
degradation in software. They widely exist in deployed software due to the 
complexity of modern software and the lack of performance testing support
~\cite{s2e,PerfBug,perf.fse10,rily.perftest,perfantipattern,xiao13:context}. 
They annoy end users and waste energy during production runs, and 
have already caused highly publicized failures~\cite{ACA-health,colorado}.
Tools that can help
developers quickly and accurately diagnose performance problems
are sorely desired.

Like general failure diagnosis, 
performance diagnosis starts from studying a problem symptom and hopefully ends
at identifying the root cause and suggesting a fix strategy. 
In the context of performance problems,
the symptom is 
execution 
slowness \cite{SongOOPSLA2014}; the root cause is about
which code region is inefficient and why.
An effective diagnosis tool can help developers quickly
and correctly figure out a fix to the performance problem.

Also like general failure diagnosis, ideal performance diagnosis tools should
satisfy three criteria.
\begin{itemize}
\item Coverage. 
Real-world performance problems are caused by a wide variety of reasons.
A good diagnosis tool should handle a good portion of them.

\item Accuracy. 
First, the inefficient code regions need to be accurately located.
Second, the reason a specific region is inefficient needs to be accurately
explained, so that developers can fix the problem.

\item Performance. 
Diagnosis often requires collecting run-time information. The lower the overhead
is, the easier for the diagnosis tool to be deployed, especially for 
production-run usage. 
\end{itemize}

No existing tools can satisfy these
three requirements.

Profiling is the state of practice in performance diagnosis.
It is far from providing the desired \textit{accuracy}, as it is designed to
tell where computation resources are spent, 
but not where and why resources are wasted. 
In many cases, the root-cause function may not even
get ranked by the profiler \cite{SongOOPSLA2014}.

Performance bug detection tools use static or dynamic analysis to identify
code regions that match specific inefficiency patterns 
\cite{Alabama,CARAMEL, Cachetor,Xu:2010:FLD:1806596.1806617,Dufour:2008:STC:1453101.1453111, Xu:2009:GFP:1542476.1542523, Xu:2010:DIC:1806596.1806616,IsilDillig.PLDI15}. 
Unfortunately, these tools are not designed for and are consequently
unsuitable for performance diagnosis.
They are not designed to provide \textit{coverage} for a wide
variety of real-world problems; their analysis is not guided by
performance failure symptom, and hence are at disadvantage in terms of diagnosis
\textit{accuracy}; dynamic detection tools often lead to 
10X slowdowns or more \cite{Cachetor,Xu:2010:FLD:1806596.1806617,Alabama}, 
not ideal in terms of \textit{performance}.

Recently, progress has been made on statistical debugging for
performance diagnosis \cite{SongOOPSLA2014}.
This approach compares runs with and without problematic performance, and
accurately identifies control-flow constructs, such as a branch $b$ or 
a loop $l$, that are most correlated with
the execution slowness.%, referred to as slowness-predictor.
Unfortunately, this approach is \textit{not} effective
for loop-related 
performance problems, which contribute to two thirds of
real-world performance problems studied in previous work 
\cite{SongOOPSLA2014,PerfBug}. It cannot
tell whether and how loop $l$ is inefficient 
and hence is not very helpful
in fixing performance problems.

\begin{figure}
\centering
\lstset{basicstyle=\ttfamily\fontsize{8}{8}\selectfont,
     morekeywords={+},keepspaces=true}
  \mbox{\lstinputlisting[mathescape,boxpos=t]{figures/GCC27733.c}}
\caption{A real-world performance bug in GCC (the `-' and `+' demonstrate the patch)}
\label{fig:GCC27733}
\end{figure}

Figure~\ref{fig:GCC27733} shows a real-world performance bug in GCC. 
Function \texttt{mult\_alg} is used to compute the best algorithm for 
multiplying \texttt{t}. It is so time consuming that developers used a
hash-table
\texttt{alg\_hash} to remember which \texttt{t} has been processed
in the past and what is the result. 
Unfortunately, a mistake in the type declaration of hash-table
entry \texttt{hash\_entry} makes the memoization useless for large \texttt{t},
severely hurting performance.
In reality, developers easily figured out that function
\texttt{mult\_alg} is most correlated with the execution slowness, but
spent several weeks to finally figure out \texttt{mult\_alg} is in fact
inefficient.
%If a tool can tell them not only which loop\footnote{Recursive functions are
%handled similarly as loops in this paper.}
%is the root cause but also why a loop is inefficient (i.e., lot of redundant
%computation and the need for better memoization in this example), 
%the diagnosis and bug fixing
%process would be much easier for developers. 

Clearly, more research is needed to improve the state of the art of performance
diagnosis --- better diagnosis
coverage, better diagnosis accuracy, and low run-time
overhead for common performance problems,
especially loop-related performance problems.


\subsection{Contributions}
This paper presents a tool, \Tool, that can help effectively diagnose
inefficient loop problems, the most common type of performance problems
\cite{SongOOPSLA2014,PerfBug}, with good coverage, accuracy, and performance.

\Tool tackles this challenging problem in three steps.
%}

First, figuring out a root-cause taxonomy for inefficient loops.
Such a taxonomy is the prerequisite to developing a general and accurate
diagnosis tool. Guided by a thorough study of 45 real-world inefficient
loop problems, we come up with a hierarchical root-cause taxonomy for
inefficient loops that are
%Specifically, we consider \textit{resultless} and
%\textit{redundancy} as two main categories of root causes for inefficient
%loops, representing loops that spend effort without producing
%any results or producing redundant results. They are further divided into
%sub-categories based on the granularity of the resultless or redundant work. 
general enough to cover
common inefficient loops, and also specific enough to help developers understand
and fix performance problems. More details are
in Section \ref{sec:study}.
 
Second, building a tool(kit) \Tool that can automatically and accurately
identify whether and how a suspicious loop is inefficient, 
together with fix-strategy suggestions. 
We achieve this following several principles:

\begin{itemize}
\item Focused checking. 
Different from performance-bug detectors that blindly checks the whole
software, \Tool focuses on loops that are
most correlated with performance symptoms. 
This focus is crucial for \Tool to achieve both high
accuracy and high coverage.

\item Root-cause taxonomy guided design. To provide the needed coverage, we follow
the root-cause taxonomy discussed above and design analysis routines for 
every root-cause sub-category. Given a candidate loop, we will 
apply a series of analysis to it to see if it matches any type of inefficiency.
%We will also put the analysis results from different analysis routines together
%to figure out the most likely root cause category for an inefficient loop.

\item Static-dynamic hybrid analysis.
As we will see, static analysis alone cannot accurately identify 
inefficiency root causes, especially because some inefficiency problems only
happen under specific workload. However, pure dynamic analysis will 
cause too large 
run-time overhead. Therefore, we take a hybrid approach to achieve both
performance and accuracy goals.
\end{itemize}

Third, using sampling to further lower the run-time overhead of \Tool, without
degrading diagnosis capability. Random sampling is a natural fit for performance
diagnosis due to the repetitive nature of inefficient code, especially
inefficient loops.

We evaluated \Tool on 18 real-world performance problems. 
Evaluation results show that \Tool can accurately identify detailed
root cause for all benchmarks and provide correct fix-strategy suggestion for
16 benchmarks. All of these are achieved with low run-time overhead.
