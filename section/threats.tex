\section{Threats to Validity}
\label{sec:threats}

\Tool does not cover all loop inefficiency problems.
For example, it does not handle performance problems caused by
cache-line false sharing in multi-core machines or lock
contention issues. It also cannot provide useful fixing 
suggestions for 1* inefficient loops, as discussed in 
Section \ref{sec:coverage}.

The static analysis in \Tool could occasionally lead to inaccurate diagnosis.
Specifically, \Tool static analysis could conservatively identify some
non-side-effect instructions, such as a write to a heap variable that
is not used after the loop, as having side effects 
(Section \ref{sec:s_workless}). 
The SE-based optimization discussed in Section \ref{sec:perf} could cause
false positives in cross-loop redundancy diagnosis.
These cases are all rare in practice and not encountered in our experiments.

Our default settings of resultless and redundancy rate thresholds 
work well in our evaluation. However, they could potentially lead to false 
positives and false negatives, as discussed in Section \ref{sec:sensi}.

The results presented in Section \ref{sec:eval_taxonomy}
and Section \ref{sec:experiment}
should be interpreted together with corresponding methodologies
and not be overly generalized.
They reflect our best effort in evaluating our root-cause
taxonomy and \Tool using a non-biased set of real-world inefficient loop 
problems that have been perceived by users and fixed by developers.
The benchmark suite covers a variety of applications, workload, 
development environments and programming languages. 
However, there are definitely uncovered cases, like problems in distributed systems
and scientific computing systems, and others. 
Furthermore, although
we did not intentionally ignore any aspect of loop-related performance problems,
some loop-related problems may never be noticed by end users
or never be fixed by developers, and hence may skip our evaluation. However,
there are no conceivable ways to study them, particularly considering that 
\Tool is designed to diagnose already-manifested performance problems not to
predict not-yet-observed problems.


\comment{
We believe that the bugs in our study provide a representative sample of the 
well-documented and
fixed performance bugs that are user-perceived and loop-related in the studied 
applications. 
Since we did not set up the root-cause taxonomy to fit particular
bugs in this bug benchmark suite, we believe our taxonomy and diagnosis
framework presented below will go beyond these sampled performance bugs. 
}
